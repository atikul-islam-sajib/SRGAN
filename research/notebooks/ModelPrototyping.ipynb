{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import zipfile\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "PROCESSED_DATA_PATH = \"../../data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(value, filename):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "    else:\n",
    "        raise Exception(\"Value or filename not provided\")\n",
    "    \n",
    "def load(filename):\n",
    "    if (filename is not None):\n",
    "        return joblib.load(filename=filename)\n",
    "    else:\n",
    "        raise Exception(\"Filename not provided\")\n",
    "    \n",
    "def weight_init(m, he_normal = False):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        if he_normal:\n",
    "            nn.init.kaiming_normal_(\n",
    "                m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        else:\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    elif classname.find(\"BatchNorm\")!= -1:\n",
    "        if he_normal:\n",
    "            nn.init.kaiming_normal_(\n",
    "                m.weight, mode=\"fan_out\", nonlinearity=\"relu\")        \n",
    "        else:\n",
    "            nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "            \n",
    "def device_init(device = \"mps\"):\n",
    "    if device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    elif device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    def __init__(self, image_path = None, in_channels = 3, batch_size = 1, image_size = 64):\n",
    "        self.image_path = image_path\n",
    "        self.batch_size = batch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.train_images = list()\n",
    "        self.train_labels = list()\n",
    "        self.test_images = list()\n",
    "        self.test_labels = list()\n",
    "\n",
    "    def image_normalized(self, lr_images=True):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                (\n",
    "                    transforms.Resize((self.image_size, self.image_size))\n",
    "                    if lr_images\n",
    "                    else transforms.Resize((self.image_size*4, self.image_size*4))\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip_file:\n",
    "                zip_file.extractall(os.path.join(RAW_DATA_PATH, \"images\"))\n",
    "        else:\n",
    "            raise Exception(\"Raw data folder not found\".capitalize())\n",
    "\n",
    "    def extract_images(self):\n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            image_path = os.path.join(RAW_DATA_PATH, \"images\")\n",
    "\n",
    "            train_images = os.path.join(image_path, os.listdir(image_path)[0])\n",
    "            test_images = os.path.join(image_path, os.listdir(image_path)[1])\n",
    "\n",
    "            for idx, path in enumerate([train_images, test_images]):\n",
    "                categories = os.listdir(path)\n",
    "\n",
    "                for category in categories:\n",
    "                    images = os.path.join(path, category)\n",
    "\n",
    "                    for image in os.listdir(images):\n",
    "                        self.image_path = os.path.join(images, image)\n",
    "                        self.image = cv2.imread(self.image_path)\n",
    "\n",
    "                        if self.image is not None:\n",
    "                            if idx == 0:\n",
    "                                self.train_images.append(\n",
    "                                    self.image_normalized(lr_images=True)(\n",
    "                                        Image.fromarray(self.image)\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                                self.train_labels.append(\n",
    "                                    self.image_normalized(lr_images=False)(\n",
    "                                        Image.fromarray(self.image)\n",
    "                                    )\n",
    "                                )\n",
    "                            else:\n",
    "                                self.test_images.append(\n",
    "                                    self.image_normalized(lr_images=True)(\n",
    "                                        Image.fromarray(self.image)\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                                self.test_labels.append(\n",
    "                                    self.image_normalized(lr_images=False)(\n",
    "                                        Image.fromarray(self.image)\n",
    "                                    )\n",
    "                                )\n",
    "                        else:\n",
    "                            continue\n",
    "            return {\n",
    "                \"train_images\": self.train_images,\n",
    "                \"train_labels\": self.train_labels,\n",
    "                \"test_images\": self.test_images,\n",
    "                \"test_labels\": self.test_labels\n",
    "                }\n",
    "        else:\n",
    "            raise Exception(\"Raw data folder not found\".capitalize())\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        try:\n",
    "            images = self.extract_images()\n",
    "        except Exception as e:\n",
    "            print(\"Error in extracting images\".capitalize())\n",
    "        else:\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=list(zip(images[\"train_images\"], images[\"train_labels\"])), batch_size=self.batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(\n",
    "                dataset=list(zip(images[\"test_images\"], images[\"test_labels\"])), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            if os.path.join(PROCESSED_DATA_PATH):\n",
    "                try:\n",
    "                    dump(\n",
    "                        value=train_dataloader, filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\"))\n",
    "                except Exception as e:\n",
    "                    print(\"Error in dumping train dataloader\".capitalize())\n",
    "\n",
    "                try:   \n",
    "                    dump(\n",
    "                        value=test_dataloader, filename=os.path.join(PROCESSED_DATA_PATH, \"test_dataloader.pkl\"))\n",
    "                except Exception as e:\n",
    "                    print(\"Error in dumping test dataloader\".capitalize())\n",
    "            else:\n",
    "                raise Exception(\"Processed data folder not found\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def display_images():\n",
    "        if os.path.exists(PROCESSED_DATA_PATH):\n",
    "            images = list()\n",
    "            labels = list()\n",
    "\n",
    "            train_images = load(filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\"))\n",
    "\n",
    "            for index, (image, label) in enumerate(train_images):\n",
    "\n",
    "                if index !=64:\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            plt.figure(figsize=(40, 25))\n",
    "\n",
    "            for index, (lr_image, hr_image) in enumerate(zip(images, labels)):\n",
    "\n",
    "                lr_image = lr_image.squeeze().permute(1, 2, 0)\n",
    "                lr_image = lr_image.cpu().detach().numpy()\n",
    "                lr_image = (lr_image - lr_image.min())/(lr_image.max() - lr_image.min())\n",
    "\n",
    "                plt.subplot(2 * 8, 2 * 8, 2 * index + 1)\n",
    "                plt.imshow(lr_image, cmap=\"gray\")\n",
    "                plt.title(\"lr_image\".lower())\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                hr_image = hr_image.squeeze().permute(1, 2, 0)\n",
    "                hr_image = hr_image.cpu().detach().numpy()\n",
    "                hr_image = (hr_image - hr_image.min())/(hr_image.max() - hr_image.min())\n",
    "\n",
    "                plt.subplot(2 * 8, 2 * 8, 2 * index + 2)\n",
    "                plt.imshow(hr_image, cmap=\"gray\")\n",
    "                plt.title(\"hr_image\".lower())\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def details_dataset():\n",
    "        if os.path.exists(PROCESSED_DATA_PATH):\n",
    "            train_dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\"))\n",
    "            test_dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"test_dataloader.pkl\"))\n",
    "\n",
    "            train = sum(data.size(0) for data, _ in train_dataloader)\n",
    "            test = sum(data.size(0) for data, _ in test_dataloader)\n",
    "\n",
    "            train_lr_images, train_hr_images = next(iter(train_dataloader))\n",
    "            test_lr_images, test_hr_images = next(iter(test_dataloader))\n",
    "\n",
    "            print(\"Train images: {}\".format(train))\n",
    "            print(\"Test images: {}\".format(test))\n",
    "\n",
    "            print(\n",
    "                \"Train: lower resolution images shape: {}\".format(\n",
    "                    train_lr_images.squeeze().size()\n",
    "                ).capitalize()\n",
    "            )\n",
    "            print(\n",
    "                \"Train: higher resolution images shape: {}\".format(\n",
    "                    train_hr_images.squeeze().size()\n",
    "                ).capitalize()\n",
    "            )\n",
    "            print(\n",
    "                \"Test: lower resolution images shape: {}\".format(\n",
    "                    test_lr_images.squeeze().size()\n",
    "                ).capitalize()\n",
    "            )\n",
    "            print(\n",
    "                \"Test: higher resolution images shape: {}\".format(\n",
    "                    test_hr_images.squeeze().size()\n",
    "                ).capitalize()\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Processed data folder not found\".capitalize())\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(image_path=\"/Users/shahmuhammadraditrahman/Desktop/brain.zip\")\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n",
    "    \n",
    "    loader.details_dataset()\n",
    "    loader.display_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, in_channels = None, out_channels = None):\n",
    "        super(InputBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.kernel_size = 9\n",
    "        self.stride = 1\n",
    "        self.padding = 4\n",
    "        \n",
    "        try:\n",
    "            self.model = self.input_block()\n",
    "        except Exception as e:\n",
    "            print(\"Input block not implemented\")\n",
    "        \n",
    "    def input_block(self):\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        layers[\"conv\"] = nn.Conv2d(\n",
    "            in_channels = self.in_channels,\n",
    "            out_channels = self.out_channels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            stride = self.stride,\n",
    "            padding = self.padding\n",
    "        )\n",
    "        layers[\"PReLU\"] = nn.PReLU()\n",
    "        \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "    def forward(self, x=None):\n",
    "        if x is not None:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            raise Exception(\"Input block not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels = None, out_channels = None, index = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.index = index\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        \n",
    "        try:\n",
    "            self.model = self.residual_block()\n",
    "        except Exception as e:\n",
    "            print(\"Residual block not implemented\")\n",
    "        \n",
    "    def residual_block(self):\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        layers[\"conv{}\".format(self.index)] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )\n",
    "        layers[\"batchnorm{}\".format(self.index)] = nn.BatchNorm2d(\n",
    "            num_features=self.out_channels)\n",
    "        \n",
    "        layers[\"PReLU{}\".format(self.index)] = nn.PReLU()\n",
    "        layers[\"conv{}\".format(self.index+1)] = nn.Conv2d(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )\n",
    "        layers[\"batchnorm{}\".format(self.index+1)] = nn.BatchNorm2d(\n",
    "            num_features=self.out_channels)\n",
    "        \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "    def forward(self, x=None):\n",
    "        if x is not None:\n",
    "            return x + self.model(x)\n",
    "        else:\n",
    "            raise Exception(\"Residual block not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleBlock(nn.Module):\n",
    "    def __init__(self, in_channels = None, out_channels = None):\n",
    "        super(MiddleBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        \n",
    "        try:\n",
    "            self.model = self.middle_block()\n",
    "        except Exception as _:\n",
    "            print(\"Middle block not implemented\")\n",
    "            \n",
    "    def middle_block(self):\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        layers[\"conv\"] = nn.Conv2d(\n",
    "            in_channels = self.in_channels,\n",
    "            out_channels = self.out_channels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            stride = self.stride,\n",
    "            padding = self.padding\n",
    "        )\n",
    "        \n",
    "        layers[\"batchnorm\"] = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "    def forward(self, x = None, skip_info = None):\n",
    "        if (x is not None) and (skip_info is not None):\n",
    "            return self.model(x) + skip_info \n",
    "        else:\n",
    "            raise Exception(\"Middle block not implemented\".capitalize())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels = None, out_channels = None, is_first_block = False, index = None):\n",
    "        super(UpSampleBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.is_first_block = is_first_block\n",
    "        self.index = index\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        self.factor = 2\n",
    "        \n",
    "        try:\n",
    "            self.model = self.up_sample_block()\n",
    "        except Exception as _:\n",
    "            print(\"Up sample block not implemented\".capitalize())\n",
    "            \n",
    "    def up_sample_block(self):\n",
    "        layers = OrderedDict()\n",
    "        layers[\"conv{}\".format(self.index)] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )\n",
    "        layers[\"pixel_shuffle{}\".format(self.index)] = nn.PixelShuffle(\n",
    "            upscale_factor=self.factor)\n",
    "        \n",
    "        if self.is_first_block:\n",
    "            layers[\"PReLU\"] = nn.PReLU()\n",
    "            \n",
    "        return nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            raise Exception(\"Up sample block not implemented\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_channels = None, out_channels = None):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.kernel_size = 9\n",
    "        self.stride = 1\n",
    "        self.padding = 4\n",
    "        \n",
    "        try:\n",
    "            self.model = self.output_block()\n",
    "        except Exception as _:\n",
    "            print(\"Output block not implemented\".capitalize())\n",
    "            \n",
    "    def output_block(self):\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        layers[\"conv\"] = nn.Conv2d(\n",
    "            in_channels = self.in_channels,\n",
    "            out_channels = self.out_channels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding = self.padding\n",
    "        )\n",
    "        layers[\"tanh\"] = nn.Tanh()\n",
    "        \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            raise Exception(\"Output block not implemented\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.num_repetitive = 16\n",
    "        \n",
    "        self.input_block = InputBlock(in_channels = 3, out_channels = 64)\n",
    "        \n",
    "        self.residual_block = nn.Sequential(\n",
    "            *[ResidualBlock(in_channels=64, out_channels=64, index=index) for index in range(self.num_repetitive)])\n",
    "        \n",
    "        self.middle_block = MiddleBlock(in_channels=64, out_channels=64)\n",
    "        \n",
    "        self.up_sample = nn.Sequential(\n",
    "            *[UpSampleBlock(in_channels=64, out_channels=256, is_first_block=is_first_block, index=index)\n",
    "            for index, is_first_block in enumerate([True, False])])\n",
    "        \n",
    "        self.out_block = OutputBlock(in_channels=64, out_channels=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            input = self.input_block(x)\n",
    "            residual = self.residual_block(input)\n",
    "            middle = self.middle_block(residual, input)\n",
    "            upsample = self.up_sample(middle)\n",
    "            output = self.out_block(upsample)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Generator not implemented\".capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(InputBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "\n",
    "        self.model = self.input_block()\n",
    "\n",
    "    def input_block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                self.padding,\n",
    "            ),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Conv2d(\n",
    "                self.out_channels,\n",
    "                self.out_channels,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                self.padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=None, out_channels=None, kernel_size=3, stride=2, padding=1\n",
    "    ):\n",
    "        super(FeatureBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.model = self.feature_block()\n",
    "\n",
    "    def feature_block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                self.padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(OutBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = out_channels\n",
    "\n",
    "        self.model = self.output_block()\n",
    "\n",
    "    def output_block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels * 8, self.in_channels * 16, self.kernel_size),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Conv2d(self.in_channels * 16, 1, self.kernel_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filters = out_channels\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        self.num_repetitive = 7\n",
    "        self.layers = []\n",
    "\n",
    "        self.input = InputBlock(\n",
    "            in_channels=self.in_channels, out_channels=self.out_channels\n",
    "        )\n",
    "\n",
    "        for index in range(self.num_repetitive):\n",
    "            if index % 2:\n",
    "                self.layers.append(\n",
    "                    FeatureBlock(\n",
    "                        in_channels=self.out_channels,\n",
    "                        out_channels=self.out_channels * 2,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                self.out_channels = self.out_channels * 2\n",
    "\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    FeatureBlock(\n",
    "                        in_channels=self.out_channels, out_channels=self.out_channels\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                self.out_channels = self.out_channels\n",
    "\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveMaxPool2d(output_size=1)\n",
    "\n",
    "        self.output = OutBlock(in_channels=self.filters, out_channels=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.input(x)\n",
    "        features = self.features(input)\n",
    "        output = self.output(self.avg_pool(features))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator(in_channels=3, out_channels=64)\n",
    "\n",
    "netD(torch.randn(1, 3, 256, 256)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor - VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, pretrained = True, is_vgg16 = False):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained\n",
    "        self.is_vgg16 = is_vgg16\n",
    "        self.num_layers = 18\n",
    "        \n",
    "        if self.is_vgg16:\n",
    "            self.model = models.vgg16(pretrained=self.pretrained)\n",
    "            \n",
    "            for params in self.model.parameters():\n",
    "                params.requires_grad = False\n",
    "                \n",
    "        else:\n",
    "            self.model = models.vgg19(pretrained=self.pretrained)\n",
    "            \n",
    "            for params in self.model.parameters():\n",
    "                params.requires_grad = False\n",
    "                \n",
    "        self.model = nn.Sequential(*list(self.model.features.children())[:self.num_layers])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            raise Exception(\"Feature Extractor not implemented\".capitalize())\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_extractor = VGG16(pretrained=True, is_vgg16=False)\n",
    "    \n",
    "    images = torch.randn(1, 3, 256, 256)\n",
    "    \n",
    "    print(feature_extractor(images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    if os.path.exists(PROCESSED_DATA_PATH):\n",
    "        train_dataloader = load(\n",
    "            filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\")\n",
    "        )\n",
    "        test_dataloader = load(\n",
    "            filename=os.path.join(PROCESSED_DATA_PATH, \"test_dataloader.pkl\")\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Dataset not found\".capitalize())\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def helpers(**kwargs):\n",
    "\n",
    "    lr = kwargs[\"lr\"]\n",
    "    beta1 = kwargs[\"beta1\"]\n",
    "    adam = kwargs[\"adam\"]\n",
    "    SGD = kwargs[\"SDG\"]\n",
    "    device = kwargs[\"device\"]\n",
    "    lr_scheduler = kwargs[\"is_lr_scheduler\"]\n",
    "\n",
    "    try:\n",
    "        train_dataloader, test_dataloader = load_dataloader()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"The exception is: \", e)\n",
    "\n",
    "    if adam:\n",
    "        try:\n",
    "            netG = Generator().to(device)\n",
    "            netD = Discriminator().to(device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"The exception caught in the section # {}\".format(e).capitalize())\n",
    "        else:\n",
    "\n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "            optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "    elif SGD:\n",
    "        try:\n",
    "            netG = Generator().to(device)\n",
    "            netD = Discriminator().to(device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"The exception caught in the section # {}\".format(e).capitalize())\n",
    "        else:\n",
    "            optimizerG = optim.SGD(netG.parameters(), lr=lr, momentum=beta1)\n",
    "            optimizerD = optim.SGD(netD.parameters(), lr=lr, momentum=beta1)\n",
    "\n",
    "    if lr_scheduler:\n",
    "        schedulerG = StepLR(\n",
    "            optimizerG,\n",
    "            step_size=10,\n",
    "            gamma=0.1,\n",
    "        )\n",
    "        schedulerD = StepLR(\n",
    "            optimizerD,\n",
    "            step_size=10,\n",
    "            gamma=0.1,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        content_loss = VGG16(pretrained=True).to(device)\n",
    "        adversarial_loss = nn.MSELoss()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"The exception caught in the section # {}\".format(e).capitalize())\n",
    "\n",
    "    return {\n",
    "        \"train_dataloader\": train_dataloader,\n",
    "        \"test_dataloader\": test_dataloader,\n",
    "        \"netG\": netG,\n",
    "        \"netD\": netD,\n",
    "        \"optimizerG\": optimizerG,\n",
    "        \"optimizerD\": optimizerD,\n",
    "        \"schedulerG\": schedulerG,\n",
    "        \"schedulerD\": schedulerD,\n",
    "        \"adversarial_loss\": adversarial_loss,\n",
    "        \"content_loss\": content_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs=100,\n",
    "        lr=0.0002,\n",
    "        device=\"mps\",\n",
    "        adam=True,\n",
    "        SDG=False,\n",
    "        beta1=0.5,\n",
    "        is_l1=False,\n",
    "        is_l2=False,\n",
    "        is_elastic_net=False,\n",
    "        is_lr_scheduler=False,\n",
    "        is_mlflow = False,\n",
    "        display=True,\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.adam = adam\n",
    "        self.SDG = SDG\n",
    "        self.beta1 = beta1\n",
    "        self.is_l1 = is_l1\n",
    "        self.is_l2 = is_l2\n",
    "        self.is_elastic_net = is_elastic_net\n",
    "        self.is_lr_scheduler = is_lr_scheduler\n",
    "        self.is_mlflow = is_mlflow\n",
    "        self.display = display\n",
    "\n",
    "    def l1(self, model):\n",
    "        if model is not None:\n",
    "            return (torch.norm(input=params, p=1) for params in model.parameters()).sum()\n",
    "        else:\n",
    "            raise Exception(\"Model not found\".capitalize())\n",
    "\n",
    "    def l2(self, model):\n",
    "        if model is not None:\n",
    "            return (torch.norm(input=params, p=2) for params in model.parameters()).sum()\n",
    "        else:\n",
    "            raise Exception(\"Model not found\".capitalize())\n",
    "\n",
    "    def elastic_net(self, model):\n",
    "        pass\n",
    "\n",
    "    def save_checkpoints(self):\n",
    "        pass\n",
    "\n",
    "    def update_discriminator_training(self):\n",
    "        pass\n",
    "\n",
    "    def update_generator_training(self):\n",
    "        pass\n",
    "\n",
    "    def save_training_images(self):\n",
    "        pass\n",
    "\n",
    "    def show_progress(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_history():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
